---
title: Array Operations
description: Element-wise operations, scalar arithmetic, and reductions in IncliArray
---

## Element-Wise Operations

IncliArray supports element-wise operations with automatic broadcasting:

### Addition

```cpp
NDArray a({2, 3});
a.fillSequential();  // [0, 1, 2; 3, 4, 5]

NDArray b({2, 3});
b.ones();  // [1, 1, 1; 1, 1, 1]

NDArray c = a + b;
c.print();
// Output:
// [1, 2, 3]
// [4, 5, 6]
```

### Subtraction

```cpp
NDArray a({2, 2});
a.fill(5.0f);  // [5, 5; 5, 5]

NDArray b({2, 2});
b.fillSequential();  // [0, 1; 2, 3]

NDArray c = a - b;
c.print();
// Output:
// [5, 4]
// [3, 2]
```

### Element-Wise Multiplication

Use `element_wise_multiply()` for Hadamard product:

```cpp
NDArray a({3, 1});
a.fillSequential();  // [0; 1; 2]

NDArray b({1, 3});
b.fill(2.0f);  // [2, 2, 2]

// Broadcasting: (3,1) * (1,3) -> (3,3)
NDArray c = a.element_wise_multiply(b);
c.print();
// Output:
// [0, 0, 0]
// [2, 2, 2]
// [4, 4, 4]
```

<Warning>
**Do not confuse** `element_wise_multiply()` with `operator*`:
- `element_wise_multiply()`: Element-wise multiplication with broadcasting
- `operator*`: 2D matrix multiplication (requires exact shape match)

See [Matrix Multiplication](/guides/matrix-multiplication) for details.
</Warning>

### Division

```cpp
NDArray a({2, 2});
a.fill(10.0f);  // [10, 10; 10, 10]

NDArray b({2, 2});
b.fillSequential();  // [0, 1; 2, 3]
b = b + 1.0f;  // [1, 2; 3, 4]

NDArray c = a / b;
c.print();
// Output:
// [10, 5]
// [3.33333, 2.5]
```

<Warning>
Division by zero emits a warning and produces `inf`:
```
Warning: Division by zero attempted. Result will be 'inf'.
```
Gradients for division by zero are skipped to avoid NaN propagation.
</Warning>

### Power Operation

Raise elements to a scalar power using `operator^`:

```cpp
NDArray a({2, 2});
a.fillSequential();  // [0, 1; 2, 3]

NDArray b = a ^ 2.0f;  // Square all elements
b.print();
// Output:
// [0, 1]
// [4, 9]

// Also works with fractional powers
NDArray c = a ^ 0.5f;  // Square root
```

## Scalar Operations

All operations support scalar versions:

```cpp
NDArray a({2, 2});
a.fillSequential();  // [0, 1; 2, 3]

// Scalar addition
NDArray b = a + 10.0f;  // [10, 11; 12, 13]

// Scalar subtraction
NDArray c = a - 5.0f;   // [-5, -4; -3, -2]

// Scalar multiplication
NDArray d = a * 2.0f;   // [0, 2; 4, 6]

// Scalar division
NDArray e = a / 2.0f;   // [0, 0.5; 1, 1.5]
```

<Tip>
Scalar operations preserve the array's shape and participate in autograd:

```cpp
NDArray x({2, 2});
x.ones();

NDArray y = x * 3.0f;
y.backward();

x.print(NDArray::PrintType::Grad);
// Output:
// [3, 3]
// [3, 3]
```
</Tip>

## Reduction Operations

### Sum All Elements

Reduce array to scalar sum:

```cpp
NDArray arr({2, 3});
arr.fillSequential();  // [0, 1, 2; 3, 4, 5]

NDArray total = arr.sum();
total.print();  // Output: [15]

std::cout << total.get({0}) << std::endl;  // Access scalar: 15
```

### Sum Along Axis

Reduce along a specific dimension:

```cpp
NDArray arr({3, 4});
arr.fillSequential();
// [0,  1,  2,  3]
// [4,  5,  6,  7]
// [8,  9, 10, 11]

// Sum along axis 0 (rows) -> shape (1, 4)
NDArray colSums = arr.sum(0);
colSums.print();  // [12, 15, 18, 21]

// Sum along axis 1 (columns) -> shape (3, 1)
NDArray rowSums = arr.sum(1);
rowSums.print();
// [6]
// [22]
// [38]
```

<Note>
Summing along an axis keeps that dimension with size 1, similar to NumPy's `keepdims=True`.

Negative axis indices are supported:
```cpp
NDArray arr({2, 3, 4});
NDArray s = arr.sum(-1);  // Sum along last axis (axis 2)
```
</Note>

## Broadcasting Behavior

All element-wise operations support broadcasting (see [Broadcasting guide](/guides/broadcasting)):

```cpp
// (3, 1) + (1, 4) -> (3, 4)
NDArray a({3, 1});
a.fillSequential();  // [[0], [1], [2]]

NDArray b({1, 4});
b.fillSequential();  // [[0, 1, 2, 3]]

NDArray c = a + b;
c.metadata(true);  // Shape: (3, 4)
c.print();
// Output:
// [0, 1, 2, 3]
// [1, 2, 3, 4]
// [2, 3, 4, 5]
```

## Autograd Support

All operations record computation graphs for backpropagation:

```cpp
NDArray x({2, 2});
x.fill(2.0f);

NDArray y({2, 2});
y.fill(3.0f);

// Build computation: z = (x * y) + 10
NDArray temp = x.element_wise_multiply(y);
NDArray z = temp + 10.0f;

z.backward();

// dz/dx = y
x.print(NDArray::PrintType::Grad);
// Output:
// [3, 3]
// [3, 3]

// dz/dy = x
y.print(NDArray::PrintType::Grad);
// Output:
// [2, 2]
// [2, 2]
```

<Tip>
Reductions also propagate gradients correctly:

```cpp
NDArray x({2, 3});
x.ones();

NDArray sum = x.sum();
sum.backward();

x.print(NDArray::PrintType::Grad);
// Output: all ones (each input contributes 1 to sum)
// [1, 1, 1]
// [1, 1, 1]
```
</Tip>

## Complete Example

<CodeGroup>
```cpp Example: Neural Network Layer
#include "NDArray.h"
#include <iostream>

int main() {
    // Input: 2 samples, 3 features
    NDArray X({2, 3});
    X.rand(-1.0f, 1.0f);
    
    // Weights: 3 inputs, 4 outputs
    NDArray W({3, 4});
    W.rand(-0.5f, 0.5f);
    
    // Bias: 1x4
    NDArray b({1, 4});
    b.zeros();
    
    // Forward pass: Y = X * W + b
    NDArray Y = X * W;  // Matrix multiplication
    Y = Y + b;          // Broadcasting addition
    
    std::cout << "Output shape: ";
    Y.metadata(true);  // Shape: (2, 4)
    
    // Compute loss (sum of outputs)
    NDArray loss = Y.sum();
    
    std::cout << "\nLoss: ";
    loss.print();
    
    // Backpropagation
    loss.backward();
    
    std::cout << "\nWeight gradients:" << std::endl;
    W.print(NDArray::PrintType::Grad);
    
    return 0;
}
```
</CodeGroup>

## Performance Notes

<Steps>
1. Element-wise operations iterate over the output shape once
2. Broadcasting computes strides to map indices efficiently
3. Non-contiguous arrays use stride-aware indexing
4. Reductions iterate over the source array shape
</Steps>

## Next Steps

<CardGroup cols={2}>
  <Card title="Broadcasting" icon="expand" href="/guides/broadcasting">
    Learn broadcasting rules and patterns
  </Card>
  <Card title="Matrix Multiplication" icon="grid-2" href="/guides/matrix-multiplication">
    2D matrix multiplication with autograd
  </Card>
</CardGroup>